{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ02ZcUJGEHVKJRvplbswO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takumi-maker/bert/blob/main/bert_%E9%A1%9E%E4%BC%BC%E6%96%87%E7%AB%A0%E5%88%A4%E5%88%A5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOnKsKLPZ9gb"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "# データセットのダウンロード\n",
        "wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
        "tar xvzf ldcc-20140209.tar.gz\n",
        "# ライブラリのインストール\n",
        "apt install aptitude swig\n",
        "aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "pip install mecab-python3==0.996.5\n",
        "#pip install mecab-python3\n",
        "pip install unidic-lite nlplot japanize-matplotlib transformers fugashi ipadic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas()\n",
        "import nlplot\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')    \n",
        "import torch\n",
        "import transformers\n",
        "from transformers import BertJapaneseTokenizer\n",
        "import logging\n",
        "logging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR) # tokenize時の警告を抑制\n",
        "def load_dataset():\n",
        "    \"\"\"データセットの読み込み\"\"\"\n",
        "    paths = []\n",
        "    for dirpath, dirnames, filenames in os.walk('./text'):\n",
        "        for file in filenames:\n",
        "            if re.match(r'.+[0-9].txt',  file):\n",
        "                #print(\"{0}\".format(file))\n",
        "                paths.append(os.path.join(dirpath, file))                \n",
        "    data = {\n",
        "        'path': [],\n",
        "        'URL': [],\n",
        "        'date': [],\n",
        "        'title': [],\n",
        "        'text': [],\n",
        "    }\n",
        "    for path in paths:\n",
        "        with open(path, 'r') as f:\n",
        "            url = f.readline().strip('¥n')\n",
        "            date = f.readline().strip('¥n')\n",
        "            title = f.readline().strip('¥n')\n",
        "            text = f.read()\n",
        "            data['path'].append(path)\n",
        "            data['URL'].append(url)\n",
        "            data['date'].append(date)\n",
        "            data['title'].append(title)\n",
        "            data['text'].append(text)\n",
        "    return pd.DataFrame(data)\n",
        "df = load_dataset()"
      ],
      "metadata": {
        "id": "TYle2YkNcVgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertExtractor:\n",
        "    \"\"\"文書特徴抽出用クラス\"\"\"\n",
        "    def __init__(self):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu' #GPUが使用可能ならGPUを使用\n",
        "        self.model_name = 'cl-tohoku/bert-base-japanese-whole-word-masking' #使用する学習済みモデル名\n",
        "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(self.model_name) #使用するBERTトークナイザー\n",
        "        self.bert_model = transformers.BertModel.from_pretrained(self.model_name) #学習済みモデル呼び出し\n",
        "        self.bert_model = self.bert_model.to(self.device)\n",
        "        self.max_len = 128 #使用する入力文書の長さ。最大512まで\n",
        "    def extract(self, sentence):\n",
        "        \"\"\"文書特徴ベクトルを抽出する\"\"\"\n",
        "        # 文書のトークナイズ\n",
        "        inp = self.tokenizer.encode(sentence)\n",
        "        len_inp = len(inp)\n",
        "        # 入力トークン数の調整\n",
        "        if len_inp >= self.max_len:\n",
        "            inputs = inp[:self.max_len]\n",
        "        else:\n",
        "            inputs = inp + [0] * (self.max_len - len_inp)\n",
        "        # モデルへ文書を入力し特徴ベクトルを取り出す\n",
        "        inputs_tensor = torch.tensor([inputs], dtype=torch.long).to(self.device)        \n",
        "        seq_out, _ = self.bert_model(inputs_tensor)\n",
        "        if torch.cuda.is_available():    \n",
        "            return seq_out[0][0].cpu().detach().numpy() # 0番目は [CLS] token, 768 dim の文章特徴量\n",
        "        else:\n",
        "            return seq_out[0][0].detach().numpy()\n",
        "            "
      ],
      "metadata": {
        "id": "shoNZjV_ceoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim_matrix(matrix):\n",
        "    \"\"\"文書間のコサイン類似度を計算し、類似度行列を返す\"\"\"\n",
        "    d = matrix @ matrix.T \n",
        "    norm = (matrix * matrix).sum(axis=1, keepdims=True) ** .5\n",
        "    return d / norm / norm.T\n",
        "bex = BertExtractor()\n",
        "df['text_feature'] = df['text'].progress_apply(lambda x: bex.extract(x)) # 文書の特徴ベクトル化\n",
        "sim = cos_sim_matrix(np.stack(df.text_feature))　 # 類似度行列"
      ],
      "metadata": {
        "id": "ZoKlXNQUclB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(n=100):\n",
        "    doc = df.sample(1)\n",
        "    doc_idx = doc.index[0]\n",
        "    sim_index = sim[doc_idx].argsort()[::-1]\n",
        "    rec_df = df.iloc[sim_index][:n]\n",
        "    rec_df['similarity'] = np.sort(sim[doc_idx])[::-1][:n]\n",
        "    return rec_df[['title', 'text', 'similarity']]\n",
        "df2 = search()"
      ],
      "metadata": {
        "id": "FInLLSl9coCi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}